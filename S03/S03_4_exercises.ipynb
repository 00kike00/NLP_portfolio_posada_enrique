{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Words, Tokens, and Regular Expressions\n",
    "## Exercises Notebook - Session 3\n",
    "\n",
    "This notebook contains exercises covering:\n",
    "- Tokenization concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Tokenization Concepts\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Types vs Instances\n",
    "\n",
    "The slides distinguish between types and instances.\n",
    "For the given text, calculate:\n",
    "1. Number of instances (total tokens)\n",
    "2. Number of types (vocabulary size |V|)\n",
    "3. Type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "text = \"the cat sat on the mat the cat was fat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Heaps' Law Demonstration\n",
    "\n",
    "The slides mention Heaps' Law: vocabulary size grows with âˆšN.\n",
    "\n",
    "Generate text of increasing length and observe vocabulary growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import random\n",
    "\n",
    "# Use a simple word list to simulate text\n",
    "word_list = ['the', 'a', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "             'cat', 'dog', 'bird', 'fish', 'tree', 'house', 'car',\n",
    "             'run', 'walk', 'jump', 'eat', 'sleep', 'read', 'write',\n",
    "             'big', 'small', 'fast', 'slow', 'red', 'blue', 'green']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: BPE Simulation\n",
    "\n",
    "The slides explain Byte Pair Encoding (BPE).\n",
    "Implement a simple BPE token learner that:\n",
    "1. Starts with character vocabulary\n",
    "2. Finds most frequent adjacent pair\n",
    "3. Merges them into a new token\n",
    "4. Repeats k times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "corpus = \"low lower newest widest\"\n",
    "\n",
    "def simple_bpe(corpus, num_merges):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Advanced Regex\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Lookahead Assertions\n",
    "\n",
    "The slides introduce lookahead: (?=pattern) and (?!pattern)\n",
    "\n",
    "Write patterns to:\n",
    "1. Find words followed by a comma (without capturing comma)\n",
    "2. Find first word of line only if it doesn't start with 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "text = \"The quick, brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Non-capturing Groups\n",
    "\n",
    "The slides explain (?:...) for grouping without capturing.\n",
    "\n",
    "Write a pattern that matches \"some cats\" or \"a few cats\" \n",
    "but only captures \"cats\" (not \"some\" or \"a few\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "texts = [\n",
    "    \"some cats like fish\",\n",
    "    \"a few cats play outside\", \n",
    "    \"some dogs bark\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: GPT-2 Pre-tokenization\n",
    "\n",
    "The slides show the GPT-2 pre-tokenization regex.\n",
    "Test the pattern and understand what each part does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m gpt2_pattern = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mll|\u001b[39m\u001b[33m'\u001b[39m\u001b[33md|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+|[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw]+\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m test = \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm learning NLP! It\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms fascinating. I\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve got 100 examples.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m pretoken = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt2_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(pretoken)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "gpt2_pattern = r\"'s|'t|'re|'ve|'m|'ll|'d|\\w+|\\d+|[^\\s\\w]+\"\n",
    "test = \"I'm learning NLP! It's fascinating. I've got 100 examples.\"\n",
    "\n",
    "pretoken = re.findall(gpt2_pattern, test)\n",
    "\n",
    "print(pretoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Morphology\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Identifying Morphemes\n",
    "\n",
    "The slides define morphemes as minimal meaning-bearing units.\n",
    "\n",
    "Write code to identify potential morphemes by finding:\n",
    "1. Common suffixes (-ed, -ing, -s, -ly, -ful)\n",
    "2. Common prefixes (un-, re-, pre-, dis-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working : (None, 'work', 'ing')\n",
      "unhappy : ('un', 'happy', None)\n",
      "carefully : (None, 'careful', 'ly')\n",
      "reworked : ('re', 'work', 'ed')\n",
      "glasses : (None, 'glasse', 's')\n",
      "preprocessing : ('pre', 'process', 'ing')\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "words = [\"working\", \"unhappy\", \"carefully\", \"reworked\", \"glasses\", \"preprocessing\"]\n",
    "\n",
    "morphemes = []\n",
    "for w in words:    \n",
    "    w_morph = re.match(r\"(un|re|pre|dis)?(.*?)(ed|ing|s|ly|full)?$\", w).groups()\n",
    "    morphemes.append(f\"{w} : {w_morph}\")\n",
    "\n",
    "for m in morphemes:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
